import yfinance as yf
import pandas as pd
import datetime
import time
from tqdm import tqdm
import logging
import requests
import os
import random
from io import StringIO

# å¼•å…¥é…ç½®
# ç¡®ä¿ä½ çš„ src/config.py é‡Œå·²ç»æœ‰äº† SP500_LIMIT, SP600_LIMIT è¿™äº›å®šä¹‰
from src.config import DATA_DIR, ETF_BLOCKLIST, PROXY_URL, DB_PATH, SP500_LIMIT, SP600_LIMIT, SP400_LIMIT, NASDAQ_LIMIT, RFR_TICKER
from src.data_manager import DataManager

# è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("InitData")

# Silencing yfinance noise
logging.getLogger("yfinance").setLevel(logging.CRITICAL)

def get_tickers_from_wiki(url, name):
    """ã€çˆ¬è™«ã€‘ä»ç»´åŸºç™¾ç§‘è·å–ä»£ç  + æ¿å— (ç¨³å¥ç‰ˆ)"""
    logger.info(f"ğŸŒ Crawling {name} from Wikipedia...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/91.0.4472.124 Safari/537.36"
    }
    
    proxies = {
        "http": os.environ.get("HTTP_PROXY", PROXY_URL),
        "https": os.environ.get("HTTPS_PROXY", PROXY_URL)
    }
    
    try:
        response = requests.get(url, headers=headers, proxies=proxies, timeout=20)
        response.raise_for_status()
        
        tables = pd.read_html(StringIO(response.text))
        
        df = None
        target_col = None
        sector_col = None
        
        # 1. Data Cleaning / Column Detection
        # æˆ‘ä»¬éœ€è¦åŒæ—¶æ‰¾åˆ° Ticker å’Œ Sector
        ticker_candidates = ['Symbol', 'Ticker', 'Ticker symbol', 'Ticker Symbol']
        sector_candidates = ['GICS Sector', 'Sector', 'GICS Sector', 'Industry'] # Wikipedia å¸¸ç”¨åˆ—å
        
        for table in tables:
            # Check Ticker
            found_ticker = None
            for cand in ticker_candidates:
                if cand in table.columns:
                    found_ticker = cand
                    break
            
            # Check Sector (Current logic: MUST find ticker, Sector is optional but preferred)
            found_sector = None
            for cand in sector_candidates:
                if cand in table.columns:
                    found_sector = cand
                    break
            
            if found_ticker:
                df = table
                target_col = found_ticker
                sector_col = found_sector # Might be None
                break
                
        if df is None:
            logger.warning(f"âš ï¸ Could not find explicit Ticker column for {name}, trying first table...")
            df = tables[0]
            target_col = df.columns[0]
            
        # 2. Extract Data
        # Returns list of dict: [{'ticker': 'AAPL', 'sector': 'Technology'}, ...]
        results = []
        
        garbage_list = [
            'CONSTITUENTS', 'EXCHANGES', 'SYMBOL', 'TICKER', 'SECURITY', 'COMPANY', 'GICS SECTOR', 
            'FOUNDATION', 'OPERATOR', 'TYPE', 'WEBSITE'
        ]
        
        for idx, row in df.iterrows():
            t = str(row[target_col])
            
            # Basic Cleaning
            t = t.replace('.', '-').replace('$', '').strip()
            
            # Garbage Filter
            if t.upper() in garbage_list: continue
            if len(t) > 5 and not t.isalpha(): continue 
            if not t: continue
            
            # Sector
            sec = "Unknown"
            if sector_col and sector_col in row:
                sec = str(row[sector_col]).strip()
            
            results.append({'ticker': t, 'sector': sec})
            
        logger.info(f"âœ… Successfully fetched {len(results)} items for {name}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to scrape {name}: {e}")
        return []

def process_single_stock(ticker, db, last_update_date=None, is_benchmark=False):
    """
    ã€ä¸‹è½½æ ¸å¿ƒã€‘å¤„ç†å•ä¸ªè‚¡ç¥¨
    å‡çº§ç‚¹ï¼šæ··åˆä¸‹è½½å¹´åº¦(Financials)å’Œå­£åº¦(Quarterly)è´¢æŠ¥ï¼Œè§£å†³å†å²æ•°æ®ä¸è¶³é—®é¢˜
    """
    try:
        # ==========================================
        # A. æ™ºèƒ½è·³è¿‡åˆ¤æ–­ (Smart Skip)
        # ==========================================
        # ä¸ºäº†ä¿®å¤æ•°æ®ç¼ºå¤±ï¼Œå»ºè®®ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å…ˆæŠŠè¿™é‡Œæ”¹çŸ­ï¼Œæˆ–è€…ç›´æ¥åˆ æ‰åº“é‡è·‘
        # è¿™é‡Œä¿ç•™ 10y çš„é•¿åº¦ä»¥ç¡®ä¿è¦†ç›– 2021 å¹´çš„å›æµ‹éœ€æ±‚
        download_period = "10y" 
        start_date = None
        
        if last_update_date:
            last_dt = datetime.datetime.strptime(last_update_date, '%Y-%m-%d')
            today_dt = datetime.datetime.now()
            days_diff = (today_dt - last_dt).days
            
            # æé€Ÿæ£€æŸ¥
            if days_diff < 1:
                return 0 
            
            # å‘¨æœ«è±å…
            if today_dt.weekday() >= 5 and days_diff <= 2: 
                return 0

            # å¢é‡æ›´æ–°
            next_day = last_dt + datetime.timedelta(days=1)
            
            # ã€CRITICAL FIXã€‘é˜²æ­¢è¯·æ±‚å½“å¤©çš„è¿˜æ²¡äº§ç”Ÿçš„æ•°æ®
            # å¦‚æœ next_day >= ä»Šå¤©ï¼Œè¯´æ˜æ˜¨å¤©çš„å·²ç»æœ‰äº†ï¼Œä»Šå¤©çš„è¿˜æ²¡æ”¶ç›˜ -> è·³è¿‡
            if next_day.date() >= datetime.datetime.now().date():
                return 0
            start_date = next_day.strftime('%Y-%m-%d')
            download_period = None 

        # Santize ticker
        original_ticker = ticker
        ticker = ticker.replace('$', '').strip() 
        if original_ticker != ticker:
            logger.info(f"ğŸ”§ Sanitized ticker: {original_ticker} -> {ticker}")

        # ==========================================
        # B. ä»·æ ¼ä¸‹è½½ (Price Data)
        # ==========================================
        # logger.debug(f"Processing: {ticker}")
        obj = yf.Ticker(ticker)

        # ã€æ–°å¢ä¿®å¤ã€‘ æ£€æŸ¥æ‹†è‚¡ (Splits)
        # å¦‚æœä¸Šæ¬¡æ›´æ–°åå‘ç”Ÿäº†æ‹†è‚¡ï¼Œå¿…é¡»å…¨é‡é‡ä¸‹ï¼Œå¦åˆ™ä»·æ ¼ä¸è¿ç»­
        if start_date:
            try:
                splits = obj.splits
                if not splits.empty:
                    # æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡æ‹†è‚¡æ—¶é—´
                    last_split_date = splits.index.max().to_pydatetime()
                    last_db_date = datetime.datetime.strptime(last_update_date, '%Y-%m-%d')
                    
                    # å¦‚æœæ‹†è‚¡å‘ç”Ÿåœ¨ä¸Šæ¬¡æ›´æ–°ä¹‹åï¼Œæˆ–è€…å°±æ˜¯åŒä¸€å¤©ï¼Œå¼ºåˆ¶é‡è·‘
                    if last_split_date >= last_db_date:
                        logger.info(f"ğŸ”„ Split detected for {ticker} on {last_split_date.date()}. Forcing full redownload.")
                        start_date = None
                        download_period = "10y"
            except Exception:
                pass # è·å–æ‹†è‚¡æ•°æ®å¤±è´¥ï¼Œå®‰å…¨èµ·è§æŒ‰åŸè®¡åˆ’è·‘ (æˆ–è€…ä¹Ÿå¯ä»¥é€‰æ‹©å¼ºåˆ¶é‡è·‘ï¼Œè¿™é‡Œå…ˆä¿å®ˆ)
        
        # åªæœ‰åœ¨ç¡®å®éœ€è¦ä¸‹è½½æ—¶æ‰è”ç½‘
        hist = pd.DataFrame()
        
        # Retry Logic (3 Attempts)
        for attempt in range(3):
            try:
                if start_date:
                    hist = obj.history(start=start_date, auto_adjust=True)
                else:
                    hist = obj.history(period=download_period, auto_adjust=True)
                
                if not hist.empty:
                    break
                
                # If empty, maybe rate limited? Wait a bit
                time.sleep(2 * (attempt + 1))
            except Exception as e:
                logger.warning(f"âš ï¸ Retry {attempt+1}/3 failed for {ticker}: {e}")
                time.sleep(3 * (attempt + 1))
            
        if not hist.empty:
            if hist.index.tz is not None:
                hist.index = hist.index.tz_localize(None)
            
            records = []
            for d, row in hist.iterrows():
                records.append((d.strftime('%Y-%m-%d'), ticker, row['Close'], row['Volume']))
            db.save_prices(records)
        
        # Benchmark æˆ– å¢é‡æ›´æ–°æ— æ•°æ®æ—¶ï¼Œç›´æ¥è¿”å›
        if is_benchmark or ticker == RFR_TICKER: return 1
        if start_date and hist.empty: return 1

        # ==========================================
        # C. è´¢æŠ¥ä¸‹è½½ (Fundamentals) - MERGED MODE
        # ==========================================
        def extract_fundamentals(fin_df, bs_df):
            """Helper to extract common dates and metrics"""
            if fin_df.empty or bs_df.empty: return []
            
            common = fin_df.columns.intersection(bs_df.columns)
            recs = []
            
            # Fetch shares once
            shares = obj.info.get('sharesOutstanding')
            if not shares: return []

            for date in common:
                try:
                    ni = fin_df.loc['Net Income', date] if 'Net Income' in fin_df.index else 0
                    rev = fin_df.loc['Total Revenue', date] if 'Total Revenue' in fin_df.index else 0
                    
                    # [New for FF5] Operating Income (RMW)
                    # Try 'Operating Income' or 'EBIT'
                    op_inc = 0
                    if 'Operating Income' in fin_df.index:
                        op_inc = fin_df.loc['Operating Income', date]
                    elif 'EBIT' in fin_df.index:
                        op_inc = fin_df.loc['EBIT', date]
                        
                    eq = 0
                    for k in ['Stockholders Equity', 'Total Stockholder Equity', 'Total Equity']:
                        if k in bs_df.index:
                            eq = bs_df.loc[k, date]
                            break
                            
                    # [New for FF5] Total Assets (CMA)
                    assets = 0
                    if 'Total Assets' in bs_df.index:
                        assets = bs_df.loc['Total Assets', date]
                    
                    # 60å¤©å‰è§†åå·®é˜²æŠ¤
                    eff_date = date + datetime.timedelta(days=60)
                    if eff_date > datetime.datetime.now(): continue
                    
                    recs.append((
                        eff_date.strftime('%Y-%m-%d'), 
                        ticker, 
                        float(ni), float(eq), float(rev), float(shares), 
                        date.strftime('%Y-%m-%d'),
                        float(assets),       # New
                        float(op_inc)        # New
                    ))
                except Exception:
                    continue
            return recs

        # 1. Get Both Sets
        q_recs = extract_fundamentals(obj.quarterly_financials, obj.quarterly_balance_sheet)
        a_recs = extract_fundamentals(obj.financials, obj.balance_sheet)
        
        # 2. Merge & Deduplicate (Prefer Quarterly if date conflict? Actually dates usually differ)
        # Use a dict to dedup by report_date
        combined = {}
        for r in a_recs + q_recs:
             # r[-1] is report_date
             combined[r[-1]] = r
             
        fund_recs = list(combined.values())
            
        if fund_recs:
            db.save_fundamentals(fund_recs)
            return 1 # æ›´æ–°æˆåŠŸ

    except Exception:
        # æ•è·æ‰€æœ‰ç½‘ç»œå¼‚å¸¸ï¼Œé˜²æ­¢å•ä¸ªè‚¡ç¥¨ä¸­æ–­æ•´ä¸ªæµç¨‹
        return -1

    return 1

def main():
    db = DataManager()
    
    print("\n" + "="*60)
    print("ğŸš€ QML Reborn: Robust Update Mode (Hybrid Fundamentals)")
    print("ğŸ“¢ Version: With Ticker Sanitization Fix (No $)")
    print("="*60)

    # 1. æ‰«æç°çŠ¶
    print("ğŸ“Š Scanning existing database...")
    existing_map = db.get_latest_dates_map()
    print(f"âœ… Found {len(existing_map)} stocks already in DB.")

    # 2. å¼ºåˆ¶æ£€æŸ¥ Benchmark (SPY)
    print("\n-------- Checking Benchmark (SPY) --------")
    spy_status = process_single_stock('SPY', db, existing_map.get('SPY'), is_benchmark=True)
    if spy_status == 0:
        print("â­ï¸  SPY is up-to-date (Skipped).")
    elif spy_status == 1:
        print("âœ… SPY Data Updated.")
    else:
        print("âš ï¸ SPY Update Failed (Check Network).")

    # 2.1 å¼ºåˆ¶æ£€æŸ¥ Risk Free Rate (^IRX)
    print("\n-------- Checking Risk Free Rate (^IRX) --------")
    rfr_status = process_single_stock(RFR_TICKER, db, existing_map.get(RFR_TICKER), is_benchmark=True)
    if rfr_status == 1:
        print("âœ… RFR Data Updated.")
    else:
        print("âš ï¸ RFR Update Failed (Check Network).")

    # 3. æŠ“å–æ­£è‚¡åå• + æ¿å—ä¿¡æ¯
    sp500_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", "S&P 500")
    if SP500_LIMIT is not None:
        print(f"ğŸš§ Test Mode: Limiting S&P 500 to first {SP500_LIMIT} stocks.")
        sp500_raw = sp500_raw[:SP500_LIMIT]

    sp600_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_600_companies", "S&P 600")
    sp400_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_400_companies", "S&P 400") # MidCap
    nasdaq_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/Nasdaq-100", "NASDAQ 100")
    
    # Merge Phase
    # Use a dict to dedup by ticker, keeping the first non-Unknown sector found
    merged_map = {}
    
    for item in sp500_raw + sp600_raw + sp400_raw + nasdaq_raw:
        t = item['ticker']
        s = item['sector']
        
        if t in ETF_BLOCKLIST: continue
        
        # If new or updating Unknown sector
        if t not in merged_map:
            merged_map[t] = s
        elif merged_map[t] == 'Unknown' and s != 'Unknown':
            merged_map[t] = s
            
    final_tickers = sorted(list(merged_map.keys()))
    
    # 3.1 ä¿å­˜ Sector ä¿¡æ¯åˆ°æ•°æ®åº“
    print(f"ğŸ’¾ Saving Sector Info for {len(final_tickers)} stocks...")
    now_str = datetime.datetime.now().strftime('%Y-%m-%d')
    info_records = []
    for t in final_tickers:
        info_records.append((t, merged_map[t], None, now_str)) # (ticker, sector, industry, date)
    
    db.save_stock_info(info_records)
    
    print(f"\nğŸ¯ Total Targets: {len(final_tickers)} stocks")
    print("-" * 60)
    
    # 4. æ‰¹é‡æ‰§è¡Œ
    counts = {'Skip':0, 'Upd':0, 'Fail':0}
    pbar = tqdm(final_tickers, unit="stock")
    
    for i, ticker in enumerate(pbar):
        last_date = existing_map.get(ticker)
        
        status = process_single_stock(ticker, db, last_update_date=last_date)
        
        if status == 0: counts['Skip'] += 1
        elif status == 1: counts['Upd'] += 1
        else: counts['Fail'] += 1
        
        pbar.set_postfix(counts)
        
        # åŠ¨æ€é™æµ
        if status == 1:
            time.sleep(random.uniform(0.3, 0.7)) 
            # æ¯ 50 ä¸ªè¯·æ±‚å¤šæ­‡ä¼š
            if counts['Upd'] % 50 == 0:
                time.sleep(2.0)

    print("\n" + "="*60)
    print("âœ… PROCESS COMPLETED!")
    print(f"   â­ï¸  Skipped (Fresh):    {counts['Skip']}")
    print(f"   â¬‡ï¸  Downloaded (New):   {counts['Upd']}")
    print(f"   âš ï¸  Failed/Error:       {counts['Fail']}")
    print("="*60)

if __name__ == "__main__":
    main()