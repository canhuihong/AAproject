import yfinance as yf
import pandas as pd
import datetime
import time
from tqdm import tqdm
import logging
import requests
import sys
import os
import random
from io import StringIO

# å¼ºåˆ¶ UTF-8 è¾“å‡º
if sys.stdout.encoding.lower() != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')


# å¼•å…¥é…ç½®
# ç¡®ä¿ä½ çš„ src/config.py é‡Œå·²ç»æœ‰äº† SP500_LIMIT, SP600_LIMIT è¿™äº›å®šä¹‰
from src.config import DATA_DIR, ETF_BLOCKLIST, PROXY_URL, DB_PATH, SP500_LIMIT, SP600_LIMIT, SP400_LIMIT, NASDAQ_LIMIT, RFR_TICKER
from src.data_manager import DataManager

# è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("InitData")

# Silencing yfinance noise
logging.getLogger("yfinance").setLevel(logging.CRITICAL)

def get_tickers_from_wiki(url, name):
    """ã€çˆ¬è™«ã€‘ä»ç»´åŸºç™¾ç§‘è·å–ä»£ç  + æ¿å— (ç¨³å¥ç‰ˆ)"""
    logger.info(f"ğŸŒ Crawling {name} from Wikipedia...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/91.0.4472.124 Safari/537.36"
    }
    
    proxies = {
        "http": os.environ.get("HTTP_PROXY", PROXY_URL),
        "https": os.environ.get("HTTPS_PROXY", PROXY_URL)
    }
    
    try:
        response = requests.get(url, headers=headers, proxies=proxies, timeout=20)
        response.raise_for_status()
        
        tables = pd.read_html(StringIO(response.text))
        
        df = None
        target_col = None
        sector_col = None
        
        # 1. Data Cleaning / Column Detection
        # æˆ‘ä»¬éœ€è¦åŒæ—¶æ‰¾åˆ° Ticker å’Œ Sector
        ticker_candidates = ['Symbol', 'Ticker', 'Ticker symbol', 'Ticker Symbol']
        sector_candidates = ['GICS Sector', 'Sector', 'GICS Sector', 'Industry'] # Wikipedia å¸¸ç”¨åˆ—å
        
        for table in tables:
            # Check Ticker
            found_ticker = None
            for cand in ticker_candidates:
                if cand in table.columns:
                    found_ticker = cand
                    break
            
            # Check Sector (Current logic: MUST find ticker, Sector is optional but preferred)
            found_sector = None
            for cand in sector_candidates:
                if cand in table.columns:
                    found_sector = cand
                    break
            
            if found_ticker:
                df = table
                target_col = found_ticker
                sector_col = found_sector # Might be None
                break
                
        if df is None:
            logger.warning(f"âš ï¸ Could not find explicit Ticker column for {name}, trying first table...")
            df = tables[0]
            target_col = df.columns[0]
            
        # 2. Extract Data
        # Returns list of dict: [{'ticker': 'AAPL', 'sector': 'Technology'}, ...]
        results = []
        
        garbage_list = [
            'CONSTITUENTS', 'EXCHANGES', 'SYMBOL', 'TICKER', 'SECURITY', 'COMPANY', 'GICS SECTOR', 
            'FOUNDATION', 'OPERATOR', 'TYPE', 'WEBSITE'
        ]
        
        for idx, row in df.iterrows():
            t = str(row[target_col])
            
            # Basic Cleaning
            t = t.replace('.', '-').replace('$', '').strip()
            
            # Garbage Filter
            if t.upper() in garbage_list: continue
            if len(t) > 5 and not t.isalpha(): continue 
            if not t: continue
            
            # Sector
            sec = "Unknown"
            if sector_col and sector_col in row:
                sec = str(row[sector_col]).strip()
            
            results.append({'ticker': t, 'sector': sec})
            
        logger.info(f"âœ… Successfully fetched {len(results)} items for {name}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to scrape {name}: {e}")
        return []

def process_single_stock(ticker, db, last_update_date=None, is_benchmark=False):
    """
    ã€ä¸‹è½½æ ¸å¿ƒã€‘å¤„ç†å•ä¸ªè‚¡ç¥¨
    å‡çº§ç‚¹ï¼šæ··åˆä¸‹è½½å¹´åº¦(Financials)å’Œå­£åº¦(Quarterly)è´¢æŠ¥ï¼Œè§£å†³å†å²æ•°æ®ä¸è¶³é—®é¢˜
    """
    try:
        # ==========================================
        # A. æ™ºèƒ½è·³è¿‡åˆ¤æ–­ (Smart Skip)
        # ==========================================
        # ä¸ºäº†ä¿®å¤æ•°æ®ç¼ºå¤±ï¼Œå»ºè®®ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å…ˆæŠŠè¿™é‡Œæ”¹çŸ­ï¼Œæˆ–è€…ç›´æ¥åˆ æ‰åº“é‡è·‘
        # è¿™é‡Œä¿ç•™ 10y çš„é•¿åº¦ä»¥ç¡®ä¿è¦†ç›– 2021 å¹´çš„å›æµ‹éœ€æ±‚
        download_period = "10y" 
        start_date = None
        
        if last_update_date:
            last_dt = datetime.datetime.strptime(last_update_date, '%Y-%m-%d')
            today_dt = datetime.datetime.now()
            days_diff = (today_dt - last_dt).days
            
            # æé€Ÿæ£€æŸ¥
            if days_diff < 1:
                return 0 
            
            # å‘¨æœ«è±å…
            if today_dt.weekday() >= 5 and days_diff <= 2: 
                return 0

            # å¢é‡æ›´æ–°
            next_day = last_dt + datetime.timedelta(days=1)
            
            # ã€CRITICAL FIXã€‘é˜²æ­¢è¯·æ±‚å½“å¤©çš„è¿˜æ²¡äº§ç”Ÿçš„æ•°æ®
            # å¦‚æœ next_day >= ä»Šå¤©ï¼Œè¯´æ˜æ˜¨å¤©çš„å·²ç»æœ‰äº†ï¼Œä»Šå¤©çš„è¿˜æ²¡æ”¶ç›˜ -> è·³è¿‡
            if next_day.date() >= datetime.datetime.now().date():
                return 0
            start_date = next_day.strftime('%Y-%m-%d')
            download_period = None 

        # Santize ticker
        original_ticker = ticker
        ticker = ticker.replace('$', '').strip() 
        if original_ticker != ticker:
            logger.info(f"ğŸ”§ Sanitized ticker: {original_ticker} -> {ticker}")

        # ==========================================
        # B. ä»·æ ¼ä¸‹è½½ (Price Data)
        # ==========================================
        # logger.debug(f"Processing: {ticker}")
        obj = yf.Ticker(ticker)

        # ã€æ–°å¢ä¿®å¤ã€‘ æ£€æŸ¥æ‹†è‚¡ (Splits)
        # å¦‚æœä¸Šæ¬¡æ›´æ–°åå‘ç”Ÿäº†æ‹†è‚¡ï¼Œå¿…é¡»å…¨é‡é‡ä¸‹ï¼Œå¦åˆ™ä»·æ ¼ä¸è¿ç»­
        if start_date:
            try:
                splits = obj.splits
                if not splits.empty:
                    # æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡æ‹†è‚¡æ—¶é—´
                    last_split_date = splits.index.max().to_pydatetime()
                    last_db_date = datetime.datetime.strptime(last_update_date, '%Y-%m-%d')
                    
                    # å¦‚æœæ‹†è‚¡å‘ç”Ÿåœ¨ä¸Šæ¬¡æ›´æ–°ä¹‹åï¼Œæˆ–è€…å°±æ˜¯åŒä¸€å¤©ï¼Œå¼ºåˆ¶é‡è·‘
                    if last_split_date >= last_db_date:
                        logger.info(f"ğŸ”„ Split detected for {ticker} on {last_split_date.date()}. Forcing full redownload.")
                        start_date = None
                        download_period = "10y"
            except Exception:
                pass # è·å–æ‹†è‚¡æ•°æ®å¤±è´¥ï¼Œå®‰å…¨èµ·è§æŒ‰åŸè®¡åˆ’è·‘ (æˆ–è€…ä¹Ÿå¯ä»¥é€‰æ‹©å¼ºåˆ¶é‡è·‘ï¼Œè¿™é‡Œå…ˆä¿å®ˆ)
        
        # åªæœ‰åœ¨ç¡®å®éœ€è¦ä¸‹è½½æ—¶æ‰è”ç½‘
        hist = pd.DataFrame()
        
        # Retry Logic (3 Attempts)
        for attempt in range(3):
            try:
                if start_date:
                    hist = obj.history(start=start_date, auto_adjust=True)
                else:
                    hist = obj.history(period=download_period, auto_adjust=True)
                
                if not hist.empty:
                    break
                
                # If empty, maybe rate limited? Wait a bit
                time.sleep(2 * (attempt + 1))
            except Exception as e:
                logger.warning(f"âš ï¸ Retry {attempt+1}/3 failed for {ticker}: {e}")
                time.sleep(3 * (attempt + 1))
            
        if not hist.empty:
            if hist.index.tz is not None:
                hist.index = hist.index.tz_localize(None)
            
            records = []
            for d, row in hist.iterrows():
                records.append((d.strftime('%Y-%m-%d'), ticker, row['Close'], row['Volume']))
            db.save_prices(records)
        
        # Benchmark æˆ– å¢é‡æ›´æ–°æ— æ•°æ®æ—¶ï¼Œç›´æ¥è¿”å›
        if is_benchmark or ticker == RFR_TICKER: return 1
        
        # [FIX] å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œä½† hist ä¸ºç©º
        # æˆ‘ä»¬éœ€è¦åŒºåˆ†: "çœŸçš„æ²¡æ•°æ® (Market Closed)" è¿˜æ˜¯ "ä¸‹è½½å¤±è´¥ (Error)"
        # ç°åœ¨çš„é€»è¾‘: å¦‚æœ hist æ˜¯ç©ºçš„ï¼Œæ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æ˜¯å› ä¸º Exception
        # å®é™…ä¸Šï¼Œä¸Šé¢ retry å¾ªç¯å¦‚æœå…¨å¤±è´¥ï¼Œhist å°±æ˜¯ emptyã€‚
        # æ›´æœ‰åŠ›çš„æ–¹å¼ï¼šå¦‚æœ hist empty ä¸” start_date å¹¶ä¸æ˜¯å¾ˆä¹…ä»¥å‰ï¼ˆæ¯”å¦‚å°±æ˜¯æ˜¨å¤©ï¼‰ï¼Œä¹Ÿè®¸ OKã€‚
        # ä½†å¦‚æœæ˜¯ Rate Limitï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½æŠ¥ Errorã€‚
        # è¿™é‡Œå…ˆä¿å®ˆä¸€ç‚¹ï¼šå¦‚æœ emptyï¼Œä¸”ä¸æ˜¯ benchmarkï¼Œè¿”å› -1 æ ‡è®° failure (é™¤éæ˜¯åˆšæ”¶ç›˜æ²¡æ•°æ®)
        if hist.empty:
            # å¦‚æœæ˜¯ä»Šå¤©æˆ–æ˜¨å¤©çš„å¢é‡ï¼Œå¯èƒ½æ˜¯è¿˜æ²¡æ”¶ç›˜ï¼Œä¸ç®—é”™
            if start_date:
                start_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')
                if (datetime.datetime.now() - start_dt).days < 2:
                    return 0 # Skip/Up-to-date
            # å…¶ä»–æƒ…å†µè§†ä¸ºå¤±è´¥
            return -1

        # ==========================================
        # C. è´¢æŠ¥ä¸‹è½½ (Fundamentals) - MERGED MODE
        # ==========================================
        def extract_fundamentals(fin_df, bs_df):
            """Helper to extract common dates and metrics"""
            if fin_df.empty or bs_df.empty: return []
            
            common = fin_df.columns.intersection(bs_df.columns)
            recs = []
            
            for date in common:
                try:
                    ni = fin_df.loc['Net Income', date] if 'Net Income' in fin_df.index else 0
                    rev = fin_df.loc['Total Revenue', date] if 'Total Revenue' in fin_df.index else 0
                    
                    # [New for FF5] Operating Income (RMW)
                    op_inc = 0
                    if 'Operating Income' in fin_df.index:
                        op_inc = fin_df.loc['Operating Income', date]
                    elif 'EBIT' in fin_df.index:
                        op_inc = fin_df.loc['EBIT', date]
                        
                    eq = 0
                    for k in ['Stockholders Equity', 'Total Stockholder Equity', 'Total Equity']:
                        if k in bs_df.index:
                            eq = bs_df.loc[k, date]
                            break
                            
                    # [New for FF5] Total Assets (CMA)
                    assets = 0
                    if 'Total Assets' in bs_df.index:
                        assets = bs_df.loc['Total Assets', date]
                        
                    # [New for Accruals] Operating Cash Flow
                    # Keys can vary: 'Operating Cash Flow', 'Total Cash From Operating Activities'
                    ocf = 0
                    for k in ['Operating Cash Flow', 'Total Cash From Operating Activities']:
                        if k in fin_df.index:
                            ocf = fin_df.loc[k, date]
                            break

                    # [OPTIMIZATION] Extract Shares from Balance Sheet
                    # keys: 'Share Issued', 'Ordinary Shares Number'
                    shares = 0
                    for k in ['Share Issued', 'Ordinary Shares Number', 'Common Stock', 'Common Stock Equity']: 
                        # Note: Common Stock Equity is $ val, not count. 'Share Issued' is count.
                        if k in bs_df.index:
                            val = bs_df.loc[k, date]
                            # Simple sanity check: shares usually > 1000
                            # Some returns string?
                            shares = float(val)
                            break
                    
                    if shares == 0:
                        # Fallback: if we can't find shares in BS, maybe it's not a common stock?
                        # For now, we record 0. Downstream might need to handle this or use last known.
                        pass

                    # 60å¤©å‰è§†åå·®é˜²æŠ¤
                    eff_date = date + datetime.timedelta(days=60)
                    if eff_date > datetime.datetime.now(): continue
                    
                    recs.append((
                        eff_date.strftime('%Y-%m-%d'), 
                        ticker, 
                        float(ni), float(eq), float(rev), float(shares), 
                        date.strftime('%Y-%m-%d'),
                        float(assets),       # New
                        float(op_inc),       # New
                        float(ocf)           # New (Cash Flow)
                    ))
                except Exception:
                    continue
            return recs

        # 1. Get Both Sets
        # Helper to merge Income & Cashflow for the "fin_df" argument
        def merge_fin_cf(fin, cf):
            if fin.empty and cf.empty: return pd.DataFrame()
            if fin.empty: return cf
            if cf.empty: return fin
            # Concatenate rows (keys)
            return pd.concat([fin, cf])

        # Merge Quarterly
        q_fin_all = merge_fin_cf(obj.quarterly_financials, obj.quarterly_cashflow)
        q_recs = extract_fundamentals(q_fin_all, obj.quarterly_balance_sheet)
        
        # Merge Annual
        a_fin_all = merge_fin_cf(obj.financials, obj.cashflow)
        a_recs = extract_fundamentals(a_fin_all, obj.balance_sheet)
        
        # 2. Merge & Deduplicate
        combined = {}
        for r in a_recs + q_recs:
             # r[-1] is report_date, r[5] is shares
             combined[r[-1]] = r
             
        fund_recs = list(combined.values())
            
        if fund_recs:
            db.save_fundamentals(fund_recs)
            return 1 # æ›´æ–°æˆåŠŸ

    except Exception:
        # æ•è·æ‰€æœ‰ç½‘ç»œå¼‚å¸¸ï¼Œé˜²æ­¢å•ä¸ªè‚¡ç¥¨ä¸­æ–­æ•´ä¸ªæµç¨‹
        return -1

    return 1

# [NEW] Worker wrapper for ThreadPool
def worker_task(args):
    ticker, db, last_date = args
    try:
        # [JITTER] Add random sleep to prevent synchronized bursts hitting API limits
        time.sleep(random.uniform(0.1, 0.5))
        res = process_single_stock(ticker, db, last_update_date=last_date)
        return res
    except Exception as e:
        logger.error(f"Worker failed for {ticker}: {e}")
        return -1

def main():
    # æ³¨æ„ï¼šåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼Œæ¯ä¸ªçº¿ç¨‹éœ€è¦ç‹¬ç«‹çš„ DB è¿æ¥ï¼Œ
    # ä½† DataManager å†…éƒ¨è®¾è®¡æ˜¯æ¯æ¬¡æ“ä½œéƒ½æ–°å»ºè¿æ¥ï¼Œæ‰€ä»¥è¿™é‡Œä¼ åŒä¸€ä¸ª db å®ä¾‹æ˜¯å®‰å…¨çš„ã€‚
    db = DataManager()
    
    print("\n" + "="*60)
    print("ğŸš€ QML Reborn: High-Speed Update Mode (Multi-threaded)")
    print("ğŸ“¢ Version: Optimized Fundamentals (No obj.info call)")
    print("="*60)

    # 1. æ‰«æç°çŠ¶
    print("ğŸ“Š Scanning existing database...")
    existing_map = db.get_latest_dates_map()
    print(f"âœ… Found {len(existing_map)} stocks already in DB.")

    # 2. å¼ºåˆ¶æ£€æŸ¥ Benchmark (SPY)
    print("\n-------- Checking Benchmark (SPY) --------")
    # SPY è¿˜æ˜¯å•çº¿ç¨‹è·‘ï¼Œç¨³ä¸€ç‚¹
    spy_status = process_single_stock('SPY', db, existing_map.get('SPY'), is_benchmark=True)
    if spy_status == 1: print("âœ… SPY Data Updated.")
    else: print("â­ï¸  SPY Skipped or Failed.")

    # 3. æŠ“å–æ­£è‚¡åå•
    # 2.1 å¼ºåˆ¶æ£€æŸ¥ Risk Free Rate (^IRX)
    print("\n-------- Checking Risk Free Rate (^IRX) --------")
    rfr_status = process_single_stock(RFR_TICKER, db, existing_map.get(RFR_TICKER), is_benchmark=True)
    if rfr_status == 1: print("âœ… RFR Data Updated.")

    # 2.2 å¼ºåˆ¶æ£€æŸ¥ Macro Indicators (^VIX, ^TNX)
    print("\n-------- Checking Macro Indicators (^VIX, ^TNX) --------")
    macro_tickers = ['^VIX', '^TNX']
    for mt in macro_tickers:
         status = process_single_stock(mt, db, existing_map.get(mt), is_benchmark=True)
         if status == 1: print(f"âœ… {mt} Updated.")
         else: print(f"â­ï¸  {mt} Skipped or Failed.")
    sp500_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", "S&P 500")
    if SP500_LIMIT is not None:
        print(f"ğŸš§ Test Mode: Limiting S&P 500 to first {SP500_LIMIT} stocks.")
        sp500_raw = sp500_raw[:SP500_LIMIT]

    sp600_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_600_companies", "S&P 600")
    sp400_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_400_companies", "S&P 400") # MidCap
    nasdaq_raw = get_tickers_from_wiki("https://en.wikipedia.org/wiki/Nasdaq-100", "NASDAQ 100")
    
    # Merge Phase
    merged_map = {}
    for item in sp500_raw + sp600_raw + sp400_raw + nasdaq_raw:
        t = item['ticker']
        s = item['sector']
        if t in ETF_BLOCKLIST: continue
        if t not in merged_map:
            merged_map[t] = s
        elif merged_map[t] == 'Unknown' and s != 'Unknown':
            merged_map[t] = s
            
    final_tickers = sorted(list(merged_map.keys()))
    
    # 3.1 ä¿å­˜ Sector ä¿¡æ¯åˆ°æ•°æ®åº“
    print(f"ğŸ’¾ Saving Sector Info for {len(final_tickers)} stocks...")
    now_str = datetime.datetime.now().strftime('%Y-%m-%d')
    info_records = []
    for t in final_tickers:
        info_records.append((t, merged_map[t], None, now_str))
    db.save_stock_info(info_records)
    
    print(f"\nğŸ¯ Total Targets: {len(final_tickers)} stocks")
    print("-" * 60)
    
    # 4. æ‰¹é‡æ‰§è¡Œ (Multithreaded)
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    counts = {'Skip':0, 'Upd':0, 'Fail':0}
    
    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    tasks = []
    for ticker in final_tickers:
        last_date = existing_map.get(ticker)
        tasks.append((ticker, db, last_date))
        
    # MAX_WORKERS: Lowered to 4 to avoid 429 Errors
    MAX_WORKERS = 4 
    
    print(f"ğŸ”¥ Starting ThreadPool with {MAX_WORKERS} workers...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all tasks
        future_to_ticker = {executor.submit(worker_task, task): task[0] for task in tasks}
        
        pbar = tqdm(total=len(tasks), unit="stock")
        
        for future in as_completed(future_to_ticker):
            ticker = future_to_ticker[future]
            try:
                status = future.result()
                if status == 0: counts['Skip'] += 1
                elif status == 1: counts['Upd'] += 1
                else: counts['Fail'] += 1
            except Exception as e:
                logger.error(f"Generate exception for {ticker}: {e}")
                counts['Fail'] += 1
                
            pbar.update(1)
            pbar.set_postfix(counts)
            
        pbar.close()

    print("\n" + "="*60)
    print("âœ… PROCESS COMPLETED!")
    print(f"   â­ï¸  Skipped (Fresh):    {counts['Skip']}")
    print(f"   â¬‡ï¸  Downloaded (New):   {counts['Upd']}")
    print(f"   âš ï¸  Failed/Error:       {counts['Fail']}")
    print("="*60)

if __name__ == "__main__":
    main()