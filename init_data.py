import yfinance as yf
import pandas as pd
import datetime
import time
import logging
import requests
import os
import random
from io import StringIO

# è¿›åº¦æ¡å…¼å®¹æ€§å¤„ç†
try:
    from tqdm import tqdm
except ImportError:
    print("å»ºè®®å®‰è£… tqdm: pip install tqdm")
    def tqdm(iterable, desc=""): return iterable

from src.config import DATA_DIR, ETF_BLOCKLIST, PROXY_URL, DB_PATH
from src.data_manager import DataManager

# è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("InitData")

# Silencing yfinance noise
logging.getLogger("yfinance").setLevel(logging.CRITICAL)

def get_tickers_from_wiki(url, name):
    """ã€çˆ¬è™«ã€‘ä»ç»´åŸºç™¾ç§‘è·å–ä»£ç  (ç¨³å¥ç‰ˆ - è‡ªåŠ¨å¯»æ‰¾æ­£ç¡®è¡¨æ ¼)"""
    logger.info(f"ğŸŒ Crawling {name} from Wikipedia...")
    
    # 1. è®¾ç½®å®Œæ•´çš„è¯·æ±‚å¤´ (ä¼ªè£…æˆæµè§ˆå™¨)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/91.0.4472.124 Safari/537.36"
    }
    
    proxies = {
        "http": os.environ.get("HTTP_PROXY", PROXY_URL),
        "https": os.environ.get("HTTPS_PROXY", PROXY_URL)
    }
    
    try:
        response = requests.get(url, headers=headers, proxies=proxies, timeout=20)
        response.raise_for_status()
        
        # è§£æè¡¨æ ¼
        tables = pd.read_html(StringIO(response.text))
        
        df = None
        target_col = None
        
        # è‡ªåŠ¨å¯»æ‰¾åŒ…å« Ticker æˆ– Symbol çš„è¡¨æ ¼
        candidates = ['Symbol', 'Ticker', 'Ticker symbol', 'Ticker Symbol']
        
        for table in tables:
            # æ£€æŸ¥åˆ—å
            for candidate in candidates:
                if candidate in table.columns:
                    df = table
                    target_col = candidate
                    break
            if df is not None:
                break
                
        if df is None:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå›é€€åˆ°ç¬¬ä¸€ä¸ªè¡¨æ ¼ (å¯èƒ½æ˜¯æ—§é€»è¾‘)
            logger.warning(f"âš ï¸ Could not find explicit Ticker column for {name}, trying first table...")
            df = tables[0]
            col_name = df.columns[0]
        else:
            col_name = target_col
            
        # æ¸…æ´—ä»£ç  (æŠŠ BRK.B è½¬ä¸º BRK-B ä»¥é€‚é… Yahoo)
        tickers = df[col_name].astype(str).str.replace('.', '-', regex=False).tolist()
        
        logger.info(f"âœ… Successfully fetched {len(tickers)} tickers for {name}")
        return tickers
        
    except Exception as e:
        logger.error(f"âŒ Failed to scrape {name}: {e}")
        return []

def process_single_stock(ticker, db, last_update_date=None, is_benchmark=False):
    """
    ã€ä¸‹è½½æ ¸å¿ƒã€‘å¤„ç†å•ä¸ªè‚¡ç¥¨ (å«æ–­ç‚¹ç»­ä¼ ã€å‘¨æœ«è·³è¿‡ã€è´¢æŠ¥æ¸…æ´—)
    è¿”å›çŠ¶æ€ç ï¼š0=è·³è¿‡, 1=æ›´æ–°, -1=å¤±è´¥
    """
    try:
        # ==========================================
        # A. æ™ºèƒ½è·³è¿‡åˆ¤æ–­ (Smart Skip)
        # ==========================================
        download_period = "5y" # é»˜è®¤ä¸‹è½½é•¿åº¦
        start_date = None
        
        if last_update_date:
            last_dt = datetime.datetime.strptime(last_update_date, '%Y-%m-%d')
            today_dt = datetime.datetime.now()
            days_diff = (today_dt - last_dt).days
            
            # 1. æé€Ÿæ£€æŸ¥ï¼š24å°æ—¶å†…æ›´æ–°è¿‡ -> ç»å¯¹è·³è¿‡
            if days_diff < 1:
                return 0 
            
            # 2. å‘¨æœ«è±å…ï¼šä»Šå¤©æ˜¯å‘¨æœ«ä¸”æ•°æ®åªæ»å1-2å¤© -> è·³è¿‡
            # (å‘¨å…­=5, å‘¨æ—¥=6)
            if today_dt.weekday() >= 5 and days_diff <= 2: 
                return 0

            # å¦åˆ™ï¼Œè®¾ç½®å¢é‡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ
            next_day = last_dt + datetime.timedelta(days=1)
            
            # ã€CRITICAL FIXã€‘é˜²æ­¢è¯·æ±‚å½“å¤©çš„è¿˜æ²¡äº§ç”Ÿçš„æ•°æ®
            # å¦‚æœ next_day >= ä»Šå¤©ï¼Œè¯´æ˜æ˜¨å¤©çš„å·²ç»æœ‰äº†ï¼Œä»Šå¤©çš„è¿˜æ²¡æ”¶ç›˜ -> è·³è¿‡
            if next_day.date() >= datetime.datetime.now().date():
                return 0
                
            start_date = next_day.strftime('%Y-%m-%d')
            download_period = None 

        # ==========================================
        # B. ä»·æ ¼ä¸‹è½½ (Price Data)
        # ==========================================
        obj = yf.Ticker(ticker)
        
        # åªæœ‰åœ¨ç¡®å®éœ€è¦ä¸‹è½½æ—¶æ‰è”ç½‘
        hist = pd.DataFrame()
        
        # Retry Logic (3 Attempts)
        for attempt in range(3):
            try:
                if start_date:
                    hist = obj.history(start=start_date, auto_adjust=True)
                else:
                    hist = obj.history(period=download_period, auto_adjust=True)
                
                if not hist.empty:
                    break
                
                # If empty, maybe rate limited? Wait a bit
                time.sleep(2 * (attempt + 1))
            except Exception as e:
                logger.warning(f"âš ï¸ Retry {attempt+1}/3 failed for {ticker}: {e}")
                time.sleep(3 * (attempt + 1))
            
        if not hist.empty:
            if hist.index.tz is not None:
                hist.index = hist.index.tz_localize(None)
            
            records = []
            for d, row in hist.iterrows():
                # å­˜å…¥æ•°æ®åº“
                records.append((d.strftime('%Y-%m-%d'), ticker, row['Close'], row['Volume']))
            db.save_prices(records)
        
        # å¦‚æœæ˜¯Benchmarkï¼Œä¸æŸ¥è´¢æŠ¥ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if is_benchmark: return 1

        # å¦‚æœå¢é‡æ›´æ–°æ—¶æ²¡ä¸‹åˆ°ä»·æ ¼(ä¾‹å¦‚ä¼‘å¸‚)ï¼Œé€šå¸¸ä¹Ÿæ— éœ€æŸ¥è´¢æŠ¥ï¼ŒèŠ‚çœæ—¶é—´
        if start_date and hist.empty: return 1

        # ==========================================
        # C. è´¢æŠ¥ä¸‹è½½ (Fundamentals) - MERGED MODE
        # ==========================================
        def extract_fundamentals(fin_df, bs_df):
            """Helper to extract common dates and metrics"""
            if fin_df.empty or bs_df.empty: return []
            
            common = fin_df.columns.intersection(bs_df.columns)
            recs = []
            
            # Fetch shares once
            shares = obj.info.get('sharesOutstanding')
            if not shares: return []

            for date in common:
                try:
                    ni = fin_df.loc['Net Income', date] if 'Net Income' in fin_df.index else 0
                    rev = fin_df.loc['Total Revenue', date] if 'Total Revenue' in fin_df.index else 0
                    
                    eq = 0
                    for k in ['Stockholders Equity', 'Total Stockholder Equity', 'Total Equity']:
                        if k in bs_df.index:
                            eq = bs_df.loc[k, date]
                            break
                    
                    # 60å¤©å‰è§†åå·®é˜²æŠ¤
                    eff_date = date + datetime.timedelta(days=60)
                    if eff_date > datetime.datetime.now(): continue
                    
                    recs.append((
                        eff_date.strftime('%Y-%m-%d'), 
                        ticker, 
                        float(ni), float(eq), float(rev), float(shares), 
                        date.strftime('%Y-%m-%d')
                    ))
                except Exception:
                    continue
            return recs

        # 1. Get Both Sets
        q_recs = extract_fundamentals(obj.quarterly_financials, obj.quarterly_balance_sheet)
        a_recs = extract_fundamentals(obj.financials, obj.balance_sheet)
        
        # 2. Merge & Deduplicate (Prefer Quarterly if date conflict? Actually dates usually differ)
        # Use a dict to dedup by report_date
        combined = {}
        for r in a_recs + q_recs:
             # r[-1] is report_date
             combined[r[-1]] = r
             
        fund_recs = list(combined.values())
            
        if fund_recs:
            db.save_fundamentals(fund_recs)
            return 1 # æ›´æ–°æˆåŠŸ

    except Exception:
        # æ•è·æ‰€æœ‰ç½‘ç»œå¼‚å¸¸ï¼Œé˜²æ­¢å•ä¸ªè‚¡ç¥¨ä¸­æ–­æ•´ä¸ªæµç¨‹
        return -1

    return 1

def main():
    db = DataManager()
    
    print("\n" + "="*60)
    print("ğŸš€ QML Reborn: Robust Update Mode (Weekends Safe)")
    print("="*60)

    # 1. æ‰«æç°çŠ¶
    print("ğŸ“Š Scanning existing database...")
    existing_map = db.get_latest_dates_map()
    print(f"âœ… Found {len(existing_map)} stocks already in DB.")

    # 2. å¼ºåˆ¶æ£€æŸ¥ Benchmark (SPY)
    print("\n-------- Checking Benchmark (SPY) --------")
    spy_status = process_single_stock('SPY', db, existing_map.get('SPY'), is_benchmark=True)
    if spy_status == 0:
        print("â­ï¸  SPY is up-to-date (Skipped).")
    elif spy_status == 1:
        print("âœ… SPY Data Updated.")
    else:
        print("âš ï¸ SPY Update Failed (Check Network).")

    # 3. æŠ“å–æ­£è‚¡åå•
    sp500 = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", "S&P 500")
    sp600 = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_600_companies", "S&P 600")
    sp400 = get_tickers_from_wiki("https://en.wikipedia.org/wiki/List_of_S%26P_400_companies", "S&P 400") # MidCap
    nasdaq100 = get_tickers_from_wiki("https://en.wikipedia.org/wiki/Nasdaq-100", "NASDAQ 100")
    
    full_list = sorted(list(set(sp500 + sp600 + sp400 + nasdaq100)))
    final_list = [t for t in full_list if t not in ETF_BLOCKLIST]
    
    print(f"\nğŸ¯ Total Targets: {len(final_list)} stocks")
    print("-" * 60)
    
    # 4. æ‰¹é‡æ‰§è¡Œ (å¸¦è®¡æ•°å™¨)
    counts = {'Skip':0, 'Upd':0, 'Fail':0}
    pbar = tqdm(final_list, unit="stock")
    
    for i, ticker in enumerate(pbar):
        last_date = existing_map.get(ticker)
        
        status = process_single_stock(ticker, db, last_update_date=last_date)
        
        if status == 0: counts['Skip'] += 1
        elif status == 1: counts['Upd'] += 1
        else: counts['Fail'] += 1
        
        # å®æ—¶æ›´æ–°è¿›åº¦æ¡åç¼€
        pbar.set_postfix(counts)
        
        # ã€æ¢å¤ã€‘ç®€å•çš„é™æµé€»è¾‘ï¼Œé˜²æ­¢ Yahoo å°ç¦
        # åªæœ‰åœ¨å‘ç”ŸçœŸå®ç½‘ç»œè¯·æ±‚(Upd)æ—¶æ‰ sleepï¼ŒSkip æ—¶ä¸ sleep
        if status == 1:
            time.sleep(random.uniform(0.3, 0.7)) 
            # æ¯ 50 ä¸ªè¯·æ±‚å¤šæ­‡ä¼š
            if counts['Upd'] % 50 == 0:
                time.sleep(2.0)

    print("\n" + "="*60)
    print("âœ… PROCESS COMPLETED!")
    print(f"   â­ï¸  Skipped (Fresh):    {counts['Skip']}")
    print(f"   â¬‡ï¸  Downloaded (New):   {counts['Upd']}")
    print(f"   âš ï¸  Failed/Error:       {counts['Fail']}")
    print("="*60)

if __name__ == "__main__":
    main()