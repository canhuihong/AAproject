import pandas as pd
import numpy as np
import logging
from src.factor_engine import FactorEngine
from src.optimizer import PortfolioOptimizer
from src.data_manager import DataManager
from src.config import BACKTEST_CONFIG, STRATEGY_PARAMS, SLIPPAGE_PARAMS, TURNOVER_PARAMS

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Backtest")

class BacktestEngine:
    def __init__(self, start_date=None, initial_capital=None, transaction_cost=None):
        self.start_date = pd.to_datetime(start_date or BACKTEST_CONFIG['START_DATE'])
        self.initial_capital = initial_capital or BACKTEST_CONFIG['INITIAL_CAPITAL']
        self.transaction_cost = transaction_cost or BACKTEST_CONFIG['TRANSACTION_COST']
        self.rebalance_freq = BACKTEST_CONFIG.get('REBALANCE_FREQ', 'M')
        self.db = DataManager()
        self.factor_engine = FactorEngine()
        
    def get_rebalance_schedule(self):
        """è·å–è°ƒä»“æ—¥åˆ—è¡¨ (æœˆæœ«)"""
        conn = self.db._get_conn()
        try:
            df_dates = pd.read_sql("SELECT DISTINCT date FROM prices ORDER BY date", conn)
        except Exception as e:
            logger.error(f"Database read error: {e}")
            return []
        finally:
            conn.close()
        
        if df_dates.empty:
            logger.error("No price data found in database!")
            return []

        df_dates['date'] = pd.to_datetime(df_dates['date'])
        
        # ç­›é€‰å›æµ‹åŒºé—´
        valid_dates = df_dates.loc[df_dates['date'] >= self.start_date, 'date']
        
        if valid_dates.empty:
            logger.warning(f"No dates found after start_date {self.start_date}")
            return []
            
        # å–æ¯ä¸ªå‘¨æœŸçš„æœ€åä¸€å¤©
        rebalance_dates = valid_dates.groupby(valid_dates.dt.to_period(self.rebalance_freq)).max()
        return rebalance_dates.sort_values().tolist()

    def _get_avg_volumes(self, tickers, anchor_date):
        """
        Fetch 20-day average Dollar Volume for slippage calc.
        Returns: dict {ticker: avg_dollar_volume}
        """
        if not tickers: return {}
        
        # Lookback 30 days to get ~20 trading days
        start_dt = (anchor_date - pd.Timedelta(days=40)).strftime('%Y-%m-%d')
        end_dt = anchor_date.strftime('%Y-%m-%d')
        
        conn = self.db._get_conn()
        placeholders = ",".join([f"'{t}'" for t in tickers])
        
        # Calculate Avg(Close * Volume) directly in SQL
        query = f"""
            SELECT ticker, AVG(close * volume) as avg_dollar_vol
            FROM prices
            WHERE ticker IN ({placeholders})
            AND date >= '{start_dt}' AND date <= '{end_dt}'
            GROUP BY ticker
        """
        try:
            df = pd.read_sql(query, conn)
            return dict(zip(df['ticker'], df['avg_dollar_vol']))
        except Exception as e:
            logger.warning(f"Error fetching volumes: {e}")
            return {}
        finally:
            conn.close()

    def _get_period_price_data(self, tickers, start_date, end_date):
        """
        è·å–ä»·æ ¼æ•°æ®ï¼Œå¹¶è¿›è¡Œé«˜å¼ºåº¦çš„æ¸…æ´—å’Œå¡«å……
        """
        conn = self.db._get_conn()
        placeholders = ",".join([f"'{t}'" for t in tickers])
        s_str = start_date.strftime('%Y-%m-%d')
        e_str = end_date.strftime('%Y-%m-%d')
        
        query = f"""
            SELECT date, ticker, close 
            FROM prices 
            WHERE ticker IN ({placeholders}) 
            AND date >= '{s_str}' AND date <= '{e_str}'
        """
        df = pd.read_sql(query, conn)
        conn.close()
        
        if df.empty: 
            return pd.DataFrame()
        
        # æ•°æ®é€è§†
        df['date'] = pd.to_datetime(df['date'])
        pivot = df.pivot(index='date', columns='ticker', values='close')
        
        # ã€å…³é”®ä¿®å¤ V2ã€‘
        # 1. æ¸©å’Œå¡«å…… (Soft Fill): å…è®¸æœ€å¤š 3 å¤©çš„æ•°æ®ç¼ºå¤± (åº”å¯¹èŠ‚å‡æ—¥æˆ–ä¸´æ—¶åœç‰Œ)
        pivot = pivot.ffill(limit=3)
        
        # 2. [Deleted] Removed pivot.bfill() to strictly prevent look-ahead bias
        # pivot = pivot.bfill()
        
        # 3. ç»Ÿè®¡å¹¶ä¸¢å¼ƒä»æœ‰ç©ºå€¼çš„åˆ—
        original_cols = len(pivot.columns)
        pivot = pivot.dropna(axis=1, how='any')
        dropped_cols = original_cols - len(pivot.columns)
        
        if dropped_cols > 0:
            logger.warning(f"   âš ï¸ Dropped {dropped_cols} stocks due to insufficient data (after soft-fill).")
        
        return pivot

    def _apply_buffer_rule(self, scored_df, current_holdings, target_size=20):
        """
        Apply Hysteresis to reduce turnover.
        Logic:
        1. Keep 'Held' stocks if Rank <= SELL_RANK (e.g. 30)
        2. Buy 'New' stocks if Rank <= BUY_RANK (e.g. 15)
        3. Sort candidates by Score and take Top N.
        """
        if not STRATEGY_PARAMS.get('ENABLE_BUFFER', False):
            return scored_df.head(target_size).index.tolist()
            
        buy_threshold = STRATEGY_PARAMS['BUFFER_BUY_RANK']
        sell_threshold = STRATEGY_PARAMS['BUFFER_SELL_RANK']
        
        candidates = []
        
        # We need the rank (1-based)
        # scored_df is already sorted by final_score descending
        
        for rank_0, (ticker, row) in enumerate(scored_df.iterrows()):
            rank = rank_0 + 1
            score = row['final_score']
            is_held = ticker in current_holdings
            
            # Rule 1: Keeper
            if is_held and rank <= sell_threshold:
                candidates.append({'ticker': ticker, 'score': score, 'is_held': True})
                
            # Rule 2: Challenger
            elif not is_held and rank <= buy_threshold:
                candidates.append({'ticker': ticker, 'score': score, 'is_held': False})
                
        # Sort by Score (High to Low)
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # Take Top N
        final_list = [x['ticker'] for x in candidates[:target_size]]
        
        return final_list

    def _apply_stop_loss(self, normalized_prices):
        """
        Apply Intra-month Trailing Stop Loss.
        If a stock drops > STOP_LOSS_PCT from its high *within this period*, 
        we simulate selling it at that price and holding cash for the rest of the period.
        """
        if not STRATEGY_PARAMS.get('ENABLE_STOP_LOSS', False):
            return normalized_prices
            
        stop_pct = STRATEGY_PARAMS['STOP_LOSS_PCT']
        
        # Calculate Drawdown from Running Max
        running_max = normalized_prices.cummax()
        drawdown = normalized_prices / running_max - 1
        
        # Identify stop trigger points
        # shape: (T, N) boolean mask
        stop_mask = drawdown < -stop_pct
        
        # We want to find the FIRST time it triggers for each stock
        # idxmax returns the index of the first True
        # but we need to check if there is ANY True
        
        modified_prices = normalized_prices.copy()
        
        for col in normalized_prices.columns:
            if stop_mask[col].any():
                # Get the first date where stop was triggered
                first_stop_date = stop_mask[col].idxmax()
                
                # Get exit price (the price at that trigger moment)
                exit_price = normalized_prices.loc[first_stop_date, col]
                
                # Lock the price at exit_price from that date onwards
                # (Simulating holding cash)
                modified_prices.loc[first_stop_date:, col] = exit_price
                
                # Log (optional, avoid spam)
                # logger.debug(f"ğŸ›‘ Trailing Stop Triggered: {col} on {first_stop_date.date()} at {exit_price:.2f}")
                
        return modified_prices

    def run(self):
        rebalance_dates = self.get_rebalance_schedule()
        if len(rebalance_dates) < 2:
            logger.warning("Not enough rebalance dates to run backtest.")
            return pd.DataFrame()
            
        logger.info(f"ğŸ“… Backtest Range: {rebalance_dates[0].date()} -> {rebalance_dates[-1].date()}")
        
        full_curve = []
        current_capital = self.initial_capital
        prev_weights = {} # ç”¨äºè®¡ç®—æ¢æ‰‹ç‡
        
        # [Optimization] Preload Data into Memory
        self.factor_engine.preload_data()
        
        # 3. é€æœŸå›æµ‹
        for i, analysis_date in enumerate(rebalance_dates[:-1]):
            curr_date = rebalance_dates[i]
            next_date = rebalance_dates[i+1]
            date_str = curr_date.strftime('%Y-%m-%d')
            
            logger.info(f"ğŸ”„ Processing {date_str} | Capital: ${current_capital:,.0f}")
            
            # --- 1. é€‰è‚¡ ---
            try:
                scored_df = self.factor_engine.get_scored_universe(analysis_date=date_str)
            except Exception as e:
                logger.warning(f"   âš ï¸ Factor Engine error on {date_str}: {e}")
                scored_df = pd.DataFrame()

            if scored_df.empty:
                logger.warning(f"   âš ï¸ No stocks selected. Holding Cash.")
                # If we haven't started trading yet, don't accumulate "flat line"
                # Just skip this date basically moving the start_date forward
                if len(full_curve) == 0:
                    logger.info("   â³ Waiting for data to warm up... Skipping period.")
                    continue
                    
                # Otherwise, hold cash
                dates = pd.date_range(curr_date, next_date)
                full_curve.append(pd.Series(current_capital, index=dates))
                continue
                
            # [MODIFIED] Stock Selection with Buffer
            current_holdings = set(prev_weights.keys())
            top_tickers = self._apply_buffer_rule(
                scored_df, 
                current_holdings, 
                target_size=BACKTEST_CONFIG.get('PORTFOLIO_SIZE', 20)
            )
            
            # --- 2. ä¼˜åŒ–æƒé‡ (Max 10%) ---
            try:
                optimizer = PortfolioOptimizer(top_tickers, analysis_date=date_str)
                # [MODIFIED] Use generic optimize_portfolio() to support HRP/MVO based on Config
                allocation_df = optimizer.optimize_portfolio()
            except Exception as e:
                logger.warning(f"   âš ï¸ Optimizer crashed: {e}")
                allocation_df = pd.DataFrame()
                
            # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œå°è¯•ç­‰æƒé‡å…œåº•
            if allocation_df.empty and top_tickers:
                logger.info("   âš ï¸ Fallback to Equal Weight.")
                allocation_df = pd.DataFrame({'ticker': top_tickers, 'weight': 1.0/len(top_tickers)})
            elif allocation_df.empty:
                continue

            # æå–æƒé‡å­—å…¸
            target_weights_df = dict(zip(allocation_df['ticker'], allocation_df['weight']))
            
            # [MODIFIED] Turnover Control (Lazy Trading)
            # If (Enable and |New - Old| < Threshold) -> Keep Old
            final_weights = {}
            if TURNOVER_PARAMS['ENABLE']:
                threshold = TURNOVER_PARAMS['TRADE_THRESHOLD']
                all_involved = set(target_weights_df.keys()) | set(prev_weights.keys())
                
                temp_weights = {}
                for t in all_involved:
                    w_new = target_weights_df.get(t, 0)
                    w_old = prev_weights.get(t, 0)
                    diff = abs(w_new - w_old)
                    
                    # Lazy Rule: If stock is held and change is small, do nothing (keep old)
                    # Note: If stock is NOT held (w_old=0), we only buy if w_new > threshold? 
                    # Usually we want to avoid small buys too.
                    
                    if diff < threshold:
                         # Keep old weight (effectively ignoring the optimization for this stock)
                         if w_old > 0:
                             temp_weights[t] = w_old
                    else:
                        # Accept new weight
                        if w_new > 0:
                            temp_weights[t] = w_new
                            
                # Renormalize 
                total_w = sum(temp_weights.values())
                if total_w > 0:
                    for t in temp_weights:
                        temp_weights[t] /= total_w  # Normalize to 1.0
                
                weights = temp_weights
            else:
                # Standard behavior
                weights = {k: v for k, v in target_weights_df.items() if v > 0.001}

            active_tickers = list(weights.keys())
            
            if not active_tickers:
                continue

            logger.info(f"   âœ… Position: {len(active_tickers)} stocks (Top: {active_tickers[:3]}...)")

            # --- 2.5 äº¤æ˜“æˆæœ¬è®¡ç®— (Dynamic Slippage) ---
            # Turnover = sum(|w_new - w_old|)
            all_tickers = list(set(weights.keys()) | set(prev_weights.keys()))
            
            slippage_cost = 0.0
            total_turnover = 0.0
            
            # Fetch Volume Data for Cost Calculation
            # We use 20-day average volume to be robust
            try:
                vol_map = self._get_avg_volumes(all_tickers, curr_date)
            except:
                vol_map = {}
            
            for t in all_tickers:
                w_new = weights.get(t, 0)
                w_old = prev_weights.get(t, 0)
                delta_w = abs(w_new - w_old)
                
                if delta_w < 1e-5: continue
                
                total_turnover += delta_w
                
                # Trade Value ($)
                trade_val = delta_w * current_capital
                
                # Dynamic Slippage Model
                # Cost = Commission + Impact
                # Impact = Base + K * sqrt(TradeVal / DailyVol)
                
                impact_bps = 0
                if SLIPPAGE_PARAMS['ENABLE']:
                    # Note: DB volume is usually shares. We need $ volume.
                    # We calculated avg_dollar_vol in SQL directly
                    avg_dollar_vol = vol_map.get(t, 10000000) # Default $10M if missing
                    
                    if avg_dollar_vol > 0:
                        participation_rate = trade_val / avg_dollar_vol
                    else:
                        participation_rate = 1.0 # High impact
                        
                    impact_bps = SLIPPAGE_PARAMS['BASE_SPREAD'] + SLIPPAGE_PARAMS['IMPACT_K'] * np.sqrt(participation_rate)
                
                # Total Rate
                total_rate = self.transaction_cost + impact_bps
                slippage_cost += trade_val * total_rate
            
            cost = slippage_cost
            current_capital -= cost
            
            logger.info(f"   ğŸ’¸ Cost: ${cost:.2f} (Turnover: {total_turnover:.1%}) -> Net Cap: ${current_capital:,.0f}")
            
            # æ›´æ–° prev_weights
            prev_weights = weights

            # --- 3. æ¨¡æ‹ŸæŒæœ‰ ---
            # ã€ä¿®å¤æœªæ¥å‡½æ•°ã€‘
            # æˆ‘ä»¬åœ¨ curr_date æ”¶ç›˜ååšå†³ç­–ï¼Œæ‰€ä»¥åœ¨ä¸‹ä¸€å¤© (curr_date + 1) å¼€å§‹æŒæœ‰
            try:
                trade_start_date = curr_date + pd.Timedelta(days=1)
                
                # è·å–ä» äº¤æ˜“æ—¥ åˆ° ä¸‹ä¸ªè°ƒä»“æ—¥ çš„æ•°æ®
                # æ³¨æ„ï¼šå¦‚æœ next_date ä¹Ÿæ˜¯ T+1ï¼Œé‚£è¿™é‡Œä¼šå–ä¸åˆ°æ•°æ®ï¼Œä½†åœ¨æœˆåº¦è°ƒä»“ä¸‹ä¸€èˆ¬æ²¡äº‹
                price_data = self._get_period_price_data(active_tickers, trade_start_date, next_date)
            except Exception as e:
                logger.warning(f"   âš ï¸ Error getting price data: {e}")
                continue
            
            # äºŒæ¬¡æ£€æŸ¥ï¼šç¡®ä¿æˆ‘ä»¬ä¹°çš„è‚¡ç¥¨åœ¨ä»·æ ¼æ•°æ®é‡ŒçœŸçš„å­˜åœ¨
            # (get_period_price_data å¯èƒ½ä¼šå› ä¸ºç¼ºæ•°æ®è€Œä¸¢å¼ƒæŸäº›åˆ—)
            valid_tickers = [t for t in active_tickers if t in price_data.columns]
            
            if not valid_tickers:
                logger.warning("   âš ï¸ No price data for selected stocks! Holding Cash.")
                dates = pd.date_range(curr_date, next_date)
                full_curve.append(pd.Series(current_capital, index=dates))
                continue
                
            # é‡æ–°å½’ä¸€åŒ–æƒé‡ (å› ä¸ºæœ‰äº›è‚¡ç¥¨å¯èƒ½è¢«ä¸¢äº†)
            valid_weights = pd.Series({t: weights[t] for t in valid_tickers})
            valid_weights = valid_weights / valid_weights.sum()
            
            # è®¡ç®—å‡€å€¼æ›²çº¿
            period_prices = price_data[valid_tickers]
            
            # å½’ä¸€åŒ–ä»·æ ¼ (Base 1.0)
            # è¿™é‡Œçš„ iloc[0] æå…¶é‡è¦ï¼Œå¿…é¡»éé›¶
            start_prices = period_prices.iloc[0]
            if (start_prices == 0).any():
                logger.warning("   âš ï¸ Found zero price, dropping bad columns.")
                period_prices = period_prices.loc[:, (start_prices != 0)]
                valid_weights = valid_weights[period_prices.columns]
                valid_weights = valid_weights / valid_weights.sum()
                start_prices = period_prices.iloc[0]

            normalized_prices = period_prices / start_prices
            
            # [MODIFIED] Apply Stop Loss
            normalized_prices = self._apply_stop_loss(normalized_prices)
            
            # æ¯æ—¥ç»„åˆä»·å€¼
            period_portfolio_value = (normalized_prices * valid_weights).sum(axis=1) * current_capital
            
            # æ‹¼æ¥
            if i > 0:
                full_curve.append(period_portfolio_value.iloc[1:]) # é¿å…æ—¥æœŸé‡å¤
            else:
                full_curve.append(period_portfolio_value)
            
            current_capital = period_portfolio_value.iloc[-1]
            
       # --- ç»“æŸ ---
        if not full_curve:
            logger.warning("Backtest produced no curve points.")
            return pd.DataFrame()
            
        equity_curve = pd.concat(full_curve)
        
        # --- ä¿®å¤ç‚¹ï¼šæ›´ç¨³å¥çš„åŸºå‡†å¯¹é½é€»è¾‘ ---
        # 1. åˆ›å»ºç»“æœ DataFrameï¼Œä»¥ç­–ç•¥ä¸ºå‡†
        result_df = pd.DataFrame({'Strategy': equity_curve})
        
        # 2. å°è¯•è·å–åŸºå‡† (SPY)
        try:
            spy_data = self._get_period_price_data(['SPY'], rebalance_dates[0], rebalance_dates[-1])
            if not spy_data.empty and 'SPY' in spy_data.columns:
                # è®¡ç®— SPY å‡€å€¼
                # å…³é”®ä¿®å¤ï¼šåŸºå‡†å¿…é¡»å’Œç­–ç•¥åœ¨åŒä¸€å¤©å½’ä¸€åŒ– (Rebase)
                # å¦åˆ™å¦‚æœç­–ç•¥ä» 2024 å¹´æ‰å¼€å§‹ (å‰é¢è·³è¿‡äº†)ï¼ŒåŸºå‡†å´ä» 2023 å¹´å¼€å§‹ç®—ï¼Œèµ·ç‚¹å°±ä¸ä¸€æ ·äº†
                spy_series = spy_data['SPY']
                
                # æ‰¾åˆ°ç­–ç•¥çœŸæ­£çš„èµ·å§‹æ—¥æœŸ
                strategy_start_date = result_df.index[0]
                
                # æˆªå–è¯¥æ—¥æœŸä¹‹åçš„ SPY
                spy_series = spy_series[spy_series.index >= strategy_start_date]
                
                if not spy_series.empty:
                    # å½’ä¸€åŒ–ï¼šè®© SPY åœ¨ç­–ç•¥å¼€å§‹çš„é‚£ä¸€å¤©ï¼Œå‡€å€¼ç­‰äºç­–ç•¥çš„åˆå§‹èµ„é‡‘
                    base_value = spy_series.iloc[0]
                    start_capital = result_df['Strategy'].iloc[0] # åº”è¯¥æ˜¯ initial_capitalï¼Œä½†ä¸ºäº†ä¿é™©å–ç¬¬ä¸€ç¬”
                    
                    spy_curve = (spy_series / base_value) * start_capital
                    
                    result_df = result_df.join(spy_curve.rename('Benchmark (SPY)'), how='left')
                    result_df['Benchmark (SPY)'] = result_df['Benchmark (SPY)'].ffill()
            else:
                logger.warning("Benchmark (SPY) data missing. Skipping benchmark comparison.")
        except Exception as e:
            logger.warning(f"Failed to process benchmark: {e}")

        # 3. æœ€ç»ˆæ¸…ç†
        # ç§»é™¤ä»»ä½•å› ä¸ºæ•°æ®æ‹¼æ¥å¯¼è‡´çš„ NaNï¼Œä½†æ‰“å°è­¦å‘Š
        original_len = len(result_df)
        final_df = result_df.dropna(subset=['Strategy']) # åªè¦ç­–ç•¥æœ‰å€¼å°±è¡Œ
        
        if len(final_df) < original_len:
             logger.warning(f"Dropped {original_len - len(final_df)} rows due to missing strategy data.")

        return final_df